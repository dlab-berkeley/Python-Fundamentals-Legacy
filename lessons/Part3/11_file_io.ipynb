{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Input/Output (I/O)\n",
    "\n",
    "**Learning Objectives**:\n",
    "\n",
    "- Read / write files using base Python and the `pandas` package.\n",
    "- Use the `os` package to loop through multiple files.\n",
    "\n",
    "****\n",
    "\n",
    "We often will need to access data in external files. In order to be able to work with the data, we'll need to **read** the data into Python. Then, after an analysis, we might need to **write** new files which contain outputs. These two tasks - file input and output - are closely related, and there's a variety of approaches to complete them in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing a Single File \n",
    "\n",
    "### Opening Files with Base Python\n",
    "\n",
    "The base Python way of importing files uses three basic steps:\n",
    "\n",
    "1. Opening the file\n",
    "2. Reading the file\n",
    "3. Closing the file\n",
    "\n",
    "Let's open the file below. What type is the `text` variable? What additional processing is needed to parse this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"capitals.csv\", \"r\")\n",
    "text = my_file.read()\n",
    "my_file.close()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening Files with `pandas`\n",
    "\n",
    "More commonly, we will use a package that does a lot of the parsing for us. The `pandas` package has a rich set of tools to help import and parse files. For example, the `pandas.read_csv()` function is particularly useful here. The [documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) shows the many arguments that can be used to customize the read_csv function. In this case, let's use the defaults for most arguments.\n",
    "\n",
    "What additional processing would be necessary to parse this file? How many rows/columns are in the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"capitals.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filepaths\n",
    "\n",
    "A **filepath** is the location of a file on your system. There are two kinds of filepaths:\n",
    "\n",
    "* **absolute**: The filepath from the top level directory.\n",
    "    * For Macs, these begin with a forward slash, e.g. `/Users/[USERNAME]/directory/subdirectory/file`.\n",
    "    * For Windows, these begin with a backward slash or, more commonly, a volume, e.g. `C:\\Documents\\directory\\subdirectory\\file`.\n",
    "* **relative**: The filepath relative to the current working directory (i.e. notebook location). Common locations include: \n",
    "    * File in same folder: `./file` or `file` (`.` means 'here').\n",
    "    * Subfolder: `subfolder/file`.\n",
    "    * Higher folder: `../sisterfolder/file` (`..` means 'go up one level in the directory').\n",
    "\n",
    "Both kinds of paths work for file names, but they will look different in the code. \n",
    "\n",
    "The above example used a relative filepath, since the file is in the same directory. Where was the file in relationship to the current working directory? \n",
    "\n",
    "What would the direct filepath be for your machine? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Subfolder Access\n",
    "\n",
    "There is a `capitals2.csv` file in the `data` subfolder of this folder. This is a common structure for projects, because it is easy to locate with a relative path, and contains only the files that will be analyzed.\n",
    "\n",
    "Each of the ways below are incorrect, and give an error. What is the error? How would you properly access the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('capitals2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../capitals2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('/data/capitals2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating through a directory\n",
    "\n",
    "The data subfolder actually has two files in it. Let's say that we want to loop through each of the files in the directory: \n",
    "\n",
    "``` \n",
    "for file in directory:\n",
    "    do something\n",
    "```\n",
    "\n",
    "One way is to make a list of relative paths, loop through the paths, and perform the operation. That would look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['data/capitals.csv', 'data/capitals2.csv']\n",
    "\n",
    "for file in files:\n",
    "    print(\"Processing filename:\", file)\n",
    "    print(pd.read_csv(file).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works for a few items, but what if we were processing dozens or hundreds of files? This would get quickly tedious. We can use the `os` package to automatically list the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filelist = os.listdir('.')\n",
    "filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful function from `os` is `os.path.join()`, which joins together a directory and a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '.'\n",
    "file = '14_Statsmodels.ipynb'\n",
    "joined = os.path.join(base_path, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_path)\n",
    "print(file)\n",
    "print(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Reading in Multiple Files\n",
    "\n",
    "Use `os.listdir()` to identify each file in the `data` subfolder. Read each `csv` file in the list using `pd.read_csv()` and append it to the variable `df_list`. \n",
    "\n",
    "**Hint**: Are there any non-csv files in the folder? What happens when you try to read them in with `read_csv()`? How can we account for them?\n",
    "\n",
    "**Bonus**: Wrap this code in a function called `read_files()` that takes as input a directory and returns a list of `pd.DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Multiple File Inputs\n",
    "\n",
    "We can combine the `pd.DataFrames` using `pd.concat`, which takes a list of data frames and stacks them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Files\n",
    "\n",
    "A `pd.DataFrame` can be exported to a csv (or other filetype) using `df.to_csv()`. This is a method function built-in to every data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! We now have covered loading in single and multiple files, and common strategies for getting data into our Python environment for processing. Now, it's possible to use this data for analysis in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
